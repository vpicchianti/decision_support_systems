{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycountry_convert\n",
    "\n",
    "# !pip install reverse_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from xml.etree import ElementTree as ET\n",
    "import reverse_geocode as rg\n",
    "import pycountry_convert as pc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_already_in_table(diz_row, table, generare_ID):\n",
    "    if not generare_ID: # ho già ID nel Police.csv, non dobbiamo crearlo noi #è il caso ad esempio di Date \n",
    "        # check se l'ID è già presente in table\n",
    "        id_to_check = None\n",
    "        for k, v in diz_row.items():  \n",
    "            if k.endswith('_fk') or k.endswith('_id'):\n",
    "                id_to_check = v  \n",
    "                break \n",
    "        if id_to_check in table.keys():\n",
    "            return id_to_check\n",
    "        return None # non è ancora presente \n",
    "     \n",
    "    else: # l'ID non è già dato ma va generato, quindi non posso fare un check su ID (non ce l'ho)\n",
    "        valori_row = list(diz_row.values())\n",
    "        for k, v in table.items():\n",
    "            if valori_row == v:\n",
    "                return int(k)\n",
    "    return None\n",
    "\n",
    "\n",
    "def add_record(diz_row, table, generare_ID):\n",
    "    id_key = None\n",
    "\n",
    "    for k, v in diz_row.items():\n",
    "        if k.endswith('_fk') or k.endswith('_id'):\n",
    "            id_key = k\n",
    "            break\n",
    "\n",
    "    if not table:\n",
    "        if generare_ID:\n",
    "            table[1] = list(diz_row.values())\n",
    "            return 1\n",
    "        else:\n",
    "            if id_key is not None:\n",
    "                id = diz_row[id_key]\n",
    "                table[id] = {k: v for k, v in diz_row.items() if k != id_key}\n",
    "                return id\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    already_in = is_already_in_table(diz_row, table, generare_ID)\n",
    "    if already_in is None:\n",
    "        if generare_ID: # adesso devo creare ID incrementale\n",
    "            pk = list(table.keys())[-1] + 1\n",
    "            table[pk] = list(diz_row.values())\n",
    "            return pk \n",
    "        else: \n",
    "            id = None\n",
    "            for k, v in diz_row.items():  \n",
    "                if k.endswith('_fk') or k.endswith('_id'):\n",
    "                    id = v  \n",
    "                    break\n",
    "            table[id] =  [{k: v for k, v in diz_row.items() if k != id_key}]\n",
    "            return id\n",
    "\n",
    "    else: # already_in not None \n",
    "\n",
    "        return already_in\n",
    "\n",
    "\n",
    "def data_into_tables(rows, tables):\n",
    "    ids = []\n",
    "    for table_name, row in zip(tables.keys(), rows):\n",
    "        table, generate_ID = tables[table_name]  # Unpack the tuple\n",
    "\n",
    "        if table_name == 'geography':\n",
    "            geo_key = (row['latitude'], row['longitude'])\n",
    "            if geo_key in table:\n",
    "                geo_fk = table[geo_key]\n",
    "            else:\n",
    "                # Generate a new geo_id if not found\n",
    "                new_geo_id = len(table) + 1\n",
    "                table[geo_key] = new_geo_id\n",
    "                geo_fk = new_geo_id\n",
    "            ids_line = geo_fk  # Assign geo_fk to ids_line\n",
    "        else:\n",
    "            ids_line = add_record(row, table, generate_ID)\n",
    "        ids.append(ids_line)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gravity(age, type, status, partecipant_info):\n",
    "    \n",
    "    ''' Crime gravity attribute is the main measure of the data warehouse. \n",
    "        You can compute its values using Eq. and the additional files .json\n",
    "        Given an instance x, \n",
    "        crime gravity(x) = F1(x.partecipant age)∗F2(x.partecipant type)∗F3(x.partecipant status)'''\n",
    "        \n",
    "\n",
    "    partecipant_age = partecipant_info.get('participant_age_group', None)\n",
    "    partecipant_type = partecipant_info.get('participant_type', None)\n",
    "    partecipant_status = partecipant_info.get('participant_status', None)\n",
    "    crime_gravity = age[partecipant_age] * type[partecipant_type] * status[partecipant_status]\n",
    "\n",
    "    return crime_gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_police(file_police, dimensional_tables, incident, custody, age, type, status): \n",
    "    with open(file_police, 'r') as police:\n",
    "        header = None\n",
    "        for line in police:\n",
    "            if header is None:\n",
    "                header = line.strip().split(',')\n",
    "            else:\n",
    "                row_values = line.strip().split(',')\n",
    "                row_dict = {header[i]: row_values[i] for i in range(len(header))}\n",
    "\n",
    "                partec_info = {\n",
    "                    'participant_age_group': row_dict['participant_age_group'],\n",
    "                    'participant_gender': row_dict['participant_gender'],\n",
    "                    'participant_status': row_dict['participant_status'],\n",
    "                    'participant_type': row_dict['participant_type']\n",
    "                }\n",
    "                geo_info = {\n",
    "                    'latitude': float(row_dict['latitude']),\n",
    "                    'longitude': float(row_dict['longitude'])\n",
    "                }\n",
    "                gun_info = {\n",
    "                    'gun_stolen': row_dict['gun_stolen'],\n",
    "                    'gun_type': row_dict['gun_type']\n",
    "                }\n",
    "                date_info = {\n",
    "                    'date_fk': int(row_dict['date_fk'])\n",
    "                }\n",
    "\n",
    "                # add data into dimension tables\n",
    "                partecipant_fk, gun_fk, date_fk, geo_fk = data_into_tables([partec_info, gun_info, geo_info, date_info],  dimensional_tables)\n",
    "                \n",
    "                # aggiungi incident info\n",
    "                incident_id = int(row_dict['incident_id'])\n",
    "                incident.add(incident_id)\n",
    "\n",
    "                # calcola crime_gravity \n",
    "                crime_gravity = compute_gravity(age, type, status, partec_info)\n",
    "                \n",
    "                custody_id = int(row_dict['custody_id'])\n",
    "                custody_record = [custody_id, partecipant_fk, gun_fk, geo_fk, date_fk,crime_gravity, incident_id] \n",
    "                custody[custody_id] = custody_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_file(file_path):\n",
    "    xml_data = []\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    for row in root.findall('row'):\n",
    "        data = {\n",
    "            'date': row.find('date').text,\n",
    "            'date_pk': row.find('date_pk').text\n",
    "        }\n",
    "        xml_data.append(data)\n",
    "    return xml_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_date(date_table, xml_data):\n",
    "    for row in xml_data:\n",
    "        date_pk = int(row['date_pk'])\n",
    "        date_info = [row['date'].split()[0], None, None, None, None, None]\n",
    "\n",
    "        try:\n",
    "            # estraggo  year, month, day \n",
    "            year, month, day = map(int, date_info[0].split('-'))\n",
    "            date_info[3] = year\n",
    "            date_info[2] = month\n",
    "            date_info[1] = day\n",
    "\n",
    "            # calcolo trimestre\n",
    "            quarter = (month - 1) // 3 + 1\n",
    "            date_info[4] = quarter\n",
    "\n",
    "            # calcolo giorno della settimana\n",
    "            datetime_obj = datetime.datetime(year, month, day)\n",
    "            day_of_week_str = datetime_obj.strftime(\"%A\")\n",
    "            date_info[5] = day_of_week_str\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        date_table[date_pk] = date_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_geography(geography):\n",
    "    def country_to_continent(country_name):\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "\n",
    "    coords = [k for k in geography.keys()]\n",
    "    results = (rg.search(coords))\n",
    "    coordinates_to_results = dict(zip(coords, results))\n",
    "\n",
    "    for coord, geo_id in geography.items():\n",
    "        if coord in coordinates_to_results:\n",
    "            result = coordinates_to_results[coord]\n",
    "            continent = country_to_continent(result['country'])\n",
    "            geography[coord] = [geo_id, result['city'], result['country'], continent] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inizializzazione delle strutture che rappresenteranno le tabelle (sia fact che dimensions)\n",
    "partecipant = {}\n",
    "geography = {}\n",
    "gun = {}\n",
    "datesprova = {}\n",
    "incident = set()\n",
    "custody = {}\n",
    "\n",
    "dimensional_tables = {\n",
    "    'partecipant': (partecipant, True) ,  # True if we need to generate the ID, False otherwise\n",
    "    'gun': (gun, True) ,\n",
    "    'geography': (geography, True) ,\n",
    "    'datesprova': (datesprova, False)\n",
    "}\n",
    "\n",
    "file_partecipant_age = 'dict_partecipant_age.json'\n",
    "file_partecipant_status = 'dict_partecipant_status.json'\n",
    "file_partecipant_type = 'dict_partecipant_type.json'\n",
    "file_police = 'Police.csv'\n",
    "file_xml = 'dates.xml'\n",
    "\n",
    "with open(file_partecipant_age, 'r') as F1:\n",
    "    age = json.load(F1)    \n",
    "with open(file_partecipant_type, 'r') as F2:\n",
    "    type = json.load(F2)\n",
    "with open(file_partecipant_status, 'r') as F3:\n",
    "    status = json.load(F3)\n",
    "\n",
    "\n",
    "process_police(file_police, dimensional_tables, incident, custody, age, type, status)\n",
    "enrich_date(datesprova, read_xml_file(file_xml))\n",
    "enrich_geography(geography)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_progetto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
